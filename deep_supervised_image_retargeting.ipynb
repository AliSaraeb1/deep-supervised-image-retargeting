{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This repo contains a PyTorch implementation of the method of the paper Deep Supervised Image Retargeting (Available [here](https://cic.tju.edu.cn/faculty/zhangjiawan/Jiawan_Zhang_files/paper/meiyijing2021.pdf)) along with the evaluation outputs. Specifically, suppose x,y, y_pred denote original, Ground Truth retargeted, and this method's retargeted image respectively and M denotes a function measuring a certain metric similarity; then we'd like to have M(y, y_pred)> M(x, y). Say the model function is G, i.e y_pred= G(x). Consider, for instance, the extreme cases of\n",
        "\n",
        " 1. G(x)= y. In this case, M(y,y_pred)= M(y,y)= infinity, i.e we always beat M(x,y), so perfect map G.\n",
        "\n",
        " 2. G(x)=x. In this case, M(y,y_pred)= M(y,x)= M(x,y), and we can never have M(y, y_pred)> M(x, y), so worst map G, and the fact that M(x, G(x))= M(x,x)= infinity is not an indication that we retargeted the image successfully.\n",
        "\n",
        " **To be able to use this code,** please download the TIRed [dataset](https://github.com/TIReD2020/TIReD), and search for the line **Path = \"/path/to/TIReD\"**, and\n",
        " **change** the path to yours.\n",
        "\n",
        " The paper presents 7 different models depending on the choice of the loss function and calls the best one \"ours\" (the one that combines all the error functions below). Thus, please **keep loss_mode = \"ours\"** (search that line) if the goal the best performing model; otherwise, **change** ours to any of \"no_Lcon\", \"no_LP\", \"no_Ltv\", \"no_Lm_tv\", \"no_L1\", \"no_LSSIM\" for benchmarking purposes.\n",
        "\n",
        "\n",
        "\n",
        " We trained the model on the TIReD dataset provided by the authors and available for download [here](https://github.com/TIReD2020/TIReD), containing the following, where train_A and test_A contain the original input images and train_B and test_B contain Ground truth retargeted images.\n"
      ],
      "metadata": {
        "id": "xPE4KYkZGhHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To run this code,** you first need to\n",
        "\n",
        "1. pip install torch torchvision piq opencv-python scikit-image matplotlib tqdm pillow\n",
        "\n",
        "2. Download the TIReD dataset [here](https://github.com/TIReD2020/TIReD). Say its path is /path/to/TIReD/  (please **modify** the line Path = \"/path/to/TIReD\" to have your path) . Then the folder looks like:"
      ],
      "metadata": {
        "id": "RzHXizlwRm0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# /path/to/TIReD/\n",
        "#   AVA/\n",
        "#     train/train_A/\n",
        "#     train/train_B/\n",
        "#     train/train_B_mask/\n",
        "#     test/test_A/\n",
        "#     test/test_B/\n",
        "#   COCO/\n",
        "#     train_224_224/train_A/\n",
        "#     train_224_224/train_B/\n",
        "#     train_224_224/train_B_mask/\n",
        "#     train_300_300/train_A/\n",
        "#     train_300_300/train_B/\n",
        "#     test/test_A/\n",
        "#     test/test_B/\n",
        "#   HKU-IS/\n",
        "#     train/train_A/\n",
        "#     train/train_B/\n",
        "#     train/train_B_mask/\n",
        "#     test/test_A/\n",
        "#     test/test_B/\n",
        "#   Waterloo Exploration/\n",
        "#     train/train_A/\n",
        "#     train/train_B/\n",
        "#     train/train_B_mask/\n",
        "#     test/test_A/\n",
        "#     test/test_B/\n"
      ],
      "metadata": {
        "id": "USY_P083H4bj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Choose which model you'd like to use from \"ours\", \"no_Lcon\", \"no_LP\", \"no_Ltv\", \"no_Lm_tv\", \"no_L1\", \"no_LSSIM\". To do so, modify the line loss_mode = \"ours\"; otherwise, **\"ours\"** is the **best performing model**.\n",
        "\n",
        "4. Finally, run python deep_supervised_image_retargeting.py. This will\n",
        "\n",
        "    a. train the model\n",
        "\n",
        "    b.  save the best checkpoint (based on highest validation SSIM metric) to mrgan_<loss_mode>_best.pth\n",
        "\n",
        "    c. use best checkpoint to evaluate the model on the test datasets\n",
        "\n",
        "    d. print the PSNR, SSIM, FSIM, VIF metrics for input vs GT and output vs GT\n",
        "\n",
        "    e. save input vs GT vs our output images to ./mrgan_examples/<dataset_name>/*.png"
      ],
      "metadata": {
        "id": "ibvWQeGHTDii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation outputs are also available in this repo."
      ],
      "metadata": {
        "id": "vteXKwi_VcFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "\n",
        "import piq\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sphiTwZaJBCO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models: \"ours\", \"no_Lcon\", \"no_LP\", \"no_Ltv\", \"no_Lm_tv\", \"no_L1\", \"no_LSSIM\"\n"
      ],
      "metadata": {
        "id": "lmvUDkEgJWHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_mode= \"ours\"\n",
        "use_Lmtv=(loss_mode!=\"no_Lm_tv\")\n",
        "use_L1=(loss_mode!=\"no_L1\")\n",
        "use_Lcon=(loss_mode!=\"no_Lcon\")\n",
        "use_LP=(loss_mode!=\"no_LP\")\n",
        "use_Ltv=(loss_mode!=\"no_Ltv\")\n",
        "use_Lssim=(loss_mode!=\"no_LSSIM\")"
      ],
      "metadata": {
        "id": "lCP74PNEJDPc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: (original image, retargeted image, mask)"
      ],
      "metadata": {
        "id": "ufmSd0RxJ6k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TIRed(Dataset):\n",
        "  def __init__(self, A, B, mask=None, image_size=224, normalize=True):\n",
        "    self.A=A\n",
        "    self.B=B\n",
        "    self.mask=mask\n",
        "    self.image_size=image_size\n",
        "\n",
        "    files=sorted(os.listdir(A))\n",
        "    self.data=[]\n",
        "    for f in files:\n",
        "      mp=None\n",
        "      if mask is not None and os.path.isdir(mask):\n",
        "        mp=os.path.join(mask, f)\n",
        "      self.data.append((os.path.join(A, f), os.path.join(B, f), mp))\n",
        "\n",
        "    #normalize\n",
        "    if normalize:\n",
        "      self.transform = T.Compose([ T.Resize((image_size, image_size)), T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "    else:\n",
        "      self.transform = T.Compose([T.Resize((image_size, image_size)), T.ToTensor()])\n",
        "    self.maskTransform = T.Compose([T.Resize((image_size, image_size), interpolation=Image.NEAREST), T.ToTensor()])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    AA, BB, MM = self.data[idx]\n",
        "\n",
        "    if MM is not None:\n",
        "      m = self.maskTransform(Image.open(MM).convert(\"L\"))\n",
        "    else:\n",
        "      m = torch.zeros(1, self.image_size, self.image_size)\n",
        "\n",
        "    return (self.transform(Image.open(AA).convert(\"RGB\")), self.transform(Image.open(BB).convert(\"RGB\")), m)\n"
      ],
      "metadata": {
        "id": "k33mpIShJ8si"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now implement the Generator and Discriminator\n",
        "\n",
        "We need this convolution layer pretty often.\n",
        "Each convolutional layer con\n",
        "tains the activation layer and a batch normalization after con\n",
        "volution. We use LeakyReLU in the encoder, and ReLU in the decoder."
      ],
      "metadata": {
        "id": "f2KmviVFLdME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ksize=3, stride=1,\n",
        "                 padding=None, dilation=1, bn=True, act=\"relu\"):\n",
        "        super().__init__()\n",
        "        if padding is None:\n",
        "            padding= (ksize // 2) * dilation\n",
        "\n",
        "        self.conv= nn.Conv2d(in_channels, out_channels, ksize, stride=stride, padding=padding, dilation=dilation, bias=not bn)\n",
        "        if bn:\n",
        "          self.bn= nn.BatchNorm2d(out_channels)\n",
        "        else:\n",
        "          self.bn =None\n",
        "\n",
        "        if act == \"relu\":\n",
        "            self.activation= nn.ReLU(inplace=True)\n",
        "        elif act == \"leaky_relu\":\n",
        "            self.activation= nn.LeakyReLU(0.2, inplace=True)\n",
        "        else:\n",
        "            self.activation= None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x= self.bn(x)\n",
        "        if self.activation is not None:\n",
        "            x= self.activation(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FLbU7V2rLkbQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder needs to be followed by a ResNet module that contains six residual blocks, so we first construct\n",
        "\n",
        "ResNet(x)= ReLU(x+ F(x))"
      ],
      "metadata": {
        "id": "6aiFTeg0MPm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.Fx = nn.Sequential(Conv(channels, channels, ksize=3, act=\"relu\"), Conv(channels, channels, ksize=3, act=None),)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y=self.Fx(x)\n",
        "        return self.activation(y + x)"
      ],
      "metadata": {
        "id": "1hvcYaXVMrla"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From shallow to deep, these blocks contain 5, 5, 4, 4 and 2 convolution layers respectively, including standard 3x3 convolutions, dilated convolutions, and 1x1 convolutions. The first four blocks end with convolution of a stride 2. In each block, the feature maps of the first convolution layer and the dilated convolution layer(s) are concatenated and we send it to the next layer."
      ],
      "metadata": {
        "id": "2azqP_lENDps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_convs, rates=(1,2,3), downsample=True):\n",
        "\n",
        "        super().__init__()\n",
        "        self.downsample= downsample\n",
        "\n",
        "        self.conv1= Conv(in_channels, out_channels, ksize=3, act=\"leaky_relu\")\n",
        "\n",
        "        self.atrous1= Conv(out_channels, out_channels, ksize=3, dilation=rates[1], act=\"leaky_relu\")\n",
        "\n",
        "        if num_convs == 5:\n",
        "          self.atrous2 = Conv(out_channels, out_channels, ksize=3, dilation=rates[2], act=\"leaky_relu\")\n",
        "        else:\n",
        "          self.atrous2= None\n",
        "\n",
        "        if num_convs >= 4:\n",
        "          self.conv1x1= Conv(out_channels * 2, out_channels, ksize=1, act=\"leaky_relu\")\n",
        "        else:\n",
        "          self.conv1x1= None\n",
        "\n",
        "        if downsample and num_convs >= 4 :\n",
        "          self.down_conv = Conv(out_channels, out_channels, ksize=3, stride=2, act=\"leaky_relu\")\n",
        "        else:\n",
        "           self.down_conv= None\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1= self.conv1(x)\n",
        "        y= self.atrous1(y1)\n",
        "        if self.atrous2 is not None:\n",
        "            y= self.atrous2(y)\n",
        "\n",
        "        if self.conv1x1 is not None:\n",
        "            y_cat= torch.cat([y1, y], dim=1)\n",
        "            y= self.conv1x1(y_cat)\n",
        "        skip= y\n",
        "\n",
        "        if self.down_conv is not None:\n",
        "            y= self.down_conv(y)\n",
        "\n",
        "        return y, skip\n"
      ],
      "metadata": {
        "id": "E4KD-N1ENK-h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is completely symmetric to the encoder and uses skip-connections to fuse multi-scale features. We use a resize-convolution scheme instead of naive deconvolution layers"
      ],
      "metadata": {
        "id": "T-xXQwjKWIuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
        "        self.conv = nn.Sequential(Conv(in_channels + skip_channels, out_channels, ksize=3, act=\"relu\"), Conv(out_channels, out_channels, ksize=3, act=\"relu\"),)\n",
        "    def forward(self, x, skip):\n",
        "        x= self.upsample(x)\n",
        "        x= torch.cat([x, skip], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nNfiDdE6WLVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator is: U-Net with a 6-block ResNet bottleneck and the 5,5,4,4,2 encoder.\n",
        "\n",
        "5 encoder blocks with channel sizes: 64, 128, 256, 512, 512\n",
        "\n",
        "6 residual blocks\n",
        "\n",
        "4 decoder blocks"
      ],
      "metadata": {
        "id": "cZIVtW_WWmTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        c1= channels  # 64\n",
        "        c2= channels * 2 # 128\n",
        "        c3= channels * 4  # 256\n",
        "        c4= channels * 8  # 512\n",
        "        c5= channels * 8  # 512\n",
        "\n",
        "        #  5 encoder blocks\n",
        "        self.enc1= Encoder(in_channels, c1, num_convs=5, rates=(1, 2, 3), downsample=True)\n",
        "        self.enc2= Encoder(c1, c2, num_convs=5, rates=(1, 2, 3), downsample=True)\n",
        "        self.enc3= Encoder(c2, c3, num_convs=4, rates=(1, 2, 3), downsample=True)\n",
        "        self.enc4= Encoder(c3, c4, num_convs=4, rates=(1, 2, 3), downsample=True)\n",
        "        self.enc5= Encoder(c4, c5, num_convs=2, rates=(1, 2, 3), downsample=False)\n",
        "\n",
        "        #  bottleneck: 6 residual blocks\n",
        "        self.bottleneck= nn.Sequential(ResNet(c5), ResNet(c5), ResNet(c5), ResNet(c5), ResNet(c5), ResNet(c5),)\n",
        "\n",
        "        # 4 decoder blocks\n",
        "        self.dec1= Decoder(in_channels=c5, skip_channels=c4, out_channels=c4)\n",
        "        self.dec2= Decoder(in_channels=c4, skip_channels=c3, out_channels=c3)\n",
        "        self.dec3= Decoder(in_channels=c3, skip_channels=c2, out_channels=c2)\n",
        "        self.dec4= Decoder(in_channels=c2, skip_channels=c1, out_channels=c1)\n",
        "\n",
        "        # 3x3 conv to RGB\n",
        "        self.final_conv = nn.Conv2d(c1, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, return_skips: bool=False):\n",
        "        y1, s1= self.enc1(x)\n",
        "        y2, s2= self.enc2(y1)\n",
        "        y3, s3= self.enc3(y2)\n",
        "        y4, s4= self.enc4(y3)\n",
        "        y5, s5= self.enc5(y4)\n",
        "\n",
        "        y= self.bottleneck(y5)\n",
        "\n",
        "        y= self.dec1(y, s4)\n",
        "        y= self.dec2(y, s3)\n",
        "        y= self.dec3(y, s2)\n",
        "        y = self.dec4(y, s1)\n",
        "\n",
        "        y= self.final_conv(y)\n",
        "\n",
        "        if return_skips:\n",
        "          return y, [s1,s2,s3,s4,s5]\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "K2XFkulpWnkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PatchGAN-style discriminator."
      ],
      "metadata": {
        "id": "ZvwFDjxgXPVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=6, channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1= Conv(in_channels, channels, ksize=4, stride=2, padding=1, bn=False, act=\"leaky_relu\")\n",
        "\n",
        "        self.block2= Conv(channels, channels * 2, ksize=4, stride=2, padding=1, bn=True, act=\"leaky_relu\")\n",
        "\n",
        "        self.block3 = Conv(channels * 2, channels * 4, ksize=4, stride=2, padding=1, bn=True, act=\"leaky_relu\")\n",
        "\n",
        "        self.block4 = Conv(channels * 4, channels * 8, ksize=4, stride=2, padding=1, bn=True, act=\"leaky_relu\")\n",
        "\n",
        "        self.final_conv= nn.Conv2d(channels * 8, 1, kernel_size=4, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y= self.block1(x)\n",
        "        y= self.block2(y)\n",
        "        y= self.block3(y)\n",
        "        y= self.block4(y)\n",
        "        y= self.final_conv(y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "pYT_9ctxXQd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement the loss functions"
      ],
      "metadata": {
        "id": "TuflDIT2Xwvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "TyxzceKLXxb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN= torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
        "\n",
        "def denorm_imagenet(x):\n",
        "    return x * IMAGENET_STD + IMAGENET_MEAN\n"
      ],
      "metadata": {
        "id": "V6C68balX4QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained VGG19 network's outputs at different blocks\n",
        "\n",
        "We will then use these features and compare them with ecoder features via loss functions\n",
        "\n",
        "block 1: 0-3\n",
        "\n",
        "block 2: 4-8\n",
        "\n",
        "block 3: 9-17\n",
        "\n",
        "block 4: 18-26\n",
        "\n",
        "block 5: 27-35"
      ],
      "metadata": {
        "id": "yy4u6xp3YEN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG19Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        VGG = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        self.features = VGG.features.eval()\n",
        "        for p in self.features.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        self.block_end = [3, 8, 17, 26, 35]\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        y= x\n",
        "        last= 0\n",
        "        for end in self.block_end:\n",
        "            for i in range(last, end + 1):\n",
        "                y= self.features[i](y)\n",
        "            features.append(y)\n",
        "            last= end + 1\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "66sx6M5GYE0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg19= VGG19Features().to(device)\n",
        "vgg19.eval()\n"
      ],
      "metadata": {
        "id": "2GyGnN_VYa9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient"
      ],
      "metadata": {
        "id": "UorNrtv2YgiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(l):\n",
        "    dy = l[:, :, 1:, :]  - l[:, :, :-1, :]\n",
        "    dx = l[:, :, :, 1:]  - l[:, :, :, :-1]\n",
        "    return dy, dx"
      ],
      "metadata": {
        "id": "hOCGD3EGYhOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L_con: Content loss (Equation 1). Compares encoder skip features and VGG features of ground truth"
      ],
      "metadata": {
        "id": "AxVJxAmzYsqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_con(skip_features, vgg_features, alpha=0.1):\n",
        "    n = min(len(skip_features), len(vgg_features))\n",
        "    if n <= 1:\n",
        "        return 0.0\n",
        "    loss= 0.0\n",
        "    for i in range(1, n):\n",
        "        loss += F.l1_loss(skip_features[i], vgg_features[i])\n",
        "    return alpha * loss / (n - 1)\n"
      ],
      "metadata": {
        "id": "_wGogBflYtIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L_tv: Total variation loss (Equation 2). Computes difference between gradients of generated image and ground truth image"
      ],
      "metadata": {
        "id": "Tq_EI-UBY4xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_tv(y_gen, y_gt):\n",
        "    dy_f, dx_f= gradient(y_gen)\n",
        "    dy_g, dx_g= gradient(y_gt)\n",
        "    return (dy_f - dy_g).abs().mean() + (dx_f - dx_g).abs().mean()\n"
      ],
      "metadata": {
        "id": "lZ2HjKkSY5eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L_mtv: Masked TV loss (eq 3). Multiplies gradient differences by the edge mask"
      ],
      "metadata": {
        "id": "t3Q1e4JiZJlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_mtv(y_gen, y_gt, mask):\n",
        "    dy_f, dx_f= gradient(y_gen)\n",
        "    dy_g, dx_g= gradient(y_gt)\n",
        "\n",
        "    m_dy= mask[:, :, 1:, :]\n",
        "    m_dx= mask[:, :, :, 1:]\n",
        "\n",
        "    m_dy= m_dy.expand_as(dy_f)\n",
        "    m_dx= m_dx.expand_as(dx_f)\n",
        "\n",
        "    return ((dy_f - dy_g).abs() * m_dy).mean() + ((dx_f - dx_g).abs() * m_dx).mean()\n"
      ],
      "metadata": {
        "id": "yoI7gHduZKMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L1: L1 reconstruction loss (eq 5). L1 distance between generated image and ground truth retargeted image."
      ],
      "metadata": {
        "id": "nbdgIhe3ZVxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L1(y_gen, y_gt):\n",
        "    return F.l1_loss(y_gen, y_gt)\n"
      ],
      "metadata": {
        "id": "jSwL4RPFZWr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L_P: Perceptual loss (eq 6). Compares VGG19 features of GT and generated images."
      ],
      "metadata": {
        "id": "bXbojKFVZd7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_P(y_gen, y_gt, beta4=1.0, beta5=2.0):\n",
        "    features_gt= vgg19(y_gt)\n",
        "    features_gen= vgg19(y_gen)\n",
        "\n",
        "    f4_gt, f5_gt= features_gt[3], features_gt[4]\n",
        "    f4_gen, f5_gen= features_gen[3], features_gen[4]\n",
        "\n",
        "    l4= F.mse_loss(f4_gen, f4_gt)\n",
        "    l5= F.mse_loss(f5_gen, f5_gt)\n",
        "\n",
        "    return beta4 * l4 + beta5 * l5\n"
      ],
      "metadata": {
        "id": "Qv4Qhz8iZj5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L_SSIM: SSIM + SSIM loss (eq 7). Finds SSIM between generated and GT images. Higher similarity means smaller loss."
      ],
      "metadata": {
        "id": "mWt0-Ux6aovb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian(size, sigma, channels, device):\n",
        "    coordinates = torch.arange(size,device=device).float()\n",
        "    coordinates -= size // 2\n",
        "    gaussian1d = torch.exp(-(coordinates ** 2) / (2 * sigma ** 2))\n",
        "    gaussian1d /= gaussian1d.sum()\n",
        "    gaussian2d= gaussian1d[:, None] @ gaussian1d[None, :]\n",
        "    gaussian2d= gaussian2d[None, None, :, :]\n",
        "    #(channels, 1, H, W)\n",
        "    gaussian2d= gaussian2d.expand(channels, 1, size, size)\n",
        "    return gaussian2d"
      ],
      "metadata": {
        "id": "N7auxi_5aqWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ssim(image1, image2, size=11, sigma=1.5):\n",
        "  C1, C2=0.01**2, 0.03**2\n",
        "  b, c, _, _=image1.shape\n",
        "  g=gaussian(size, sigma, c, image1.device)\n",
        "\n",
        "  mu1=F.conv2d(image1, g, padding=size//2, groups=c)\n",
        "  mu2=F.conv2d(image2, g, padding=size//2, groups=c)\n",
        "\n",
        "  mu1_sq, mu2_sq=mu1**2, mu2**2\n",
        "  mu1mu2=mu1*mu2\n",
        "\n",
        "  sigma1=F.conv2d(image1*image1, g, padding=size//2, groups=c)-mu1_sq\n",
        "  sigma2=F.conv2d(image2*image2, g, padding=size//2, groups=c)-mu2_sq\n",
        "  sigma12=F.conv2d(image1*image2, g, padding=size//2, groups=c)-mu1mu2\n",
        "\n",
        "  top=(2*mu1mu2+C1)*(2*sigma12+C2)\n",
        "  bottom=(mu1_sq+mu2_sq+C1)*(sigma1+sigma2+C2)\n",
        "  ssim0=top/bottom\n",
        "  return ssim0.mean()"
      ],
      "metadata": {
        "id": "q4El_8KNgX-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_SSIM(y_gen, y_gt):\n",
        "  return 1.0-ssim(torch.clamp(denorm_imagenet(y_gen), 0.0, 1.0), torch.clamp(denorm_imagenet(y_gt), 0.0, 1.0))"
      ],
      "metadata": {
        "id": "ldLljlzTkq-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L_D: binary cross entropy on the discriminator outputs.\n",
        "\n",
        "L_GAN: GAN losses (eq 4). Generator tries to fool D"
      ],
      "metadata": {
        "id": "edOH8c3Clc7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def L_D(d_real, d_gen):\n",
        "    return bce(d_real, torch.ones_like(d_real)) + bce(d_gen, torch.zeros_like(d_gen))\n",
        "\n",
        "def L_GAN(d_gen):\n",
        "    return bce(d_gen, torch.ones_like(d_gen))\n"
      ],
      "metadata": {
        "id": "Qb5JZlkRlebJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L_G: Full generator loss (eq 8). Combines the loss functions above"
      ],
      "metadata": {
        "id": "FJipN8aqm-R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Alpha_Lcon= 0.1\n",
        "Lambda_L1= 1000.0\n",
        "Lambda_P= 10.0\n",
        "Lambda_Tv= 0.01\n",
        "Lambda_Mtv= 0.003\n",
        "Lambda_Ssim= 70.0\n",
        "\n",
        "def L_G(y_gen, y_gt, mask, d_gen, skip_feats):\n",
        "  zero=y_gen.new_zeros(())\n",
        "\n",
        "  L_adv=L_GAN(d_gen)\n",
        "\n",
        "  if use_Lcon:\n",
        "    vgg_gt=vgg19(y_gt)\n",
        "    Lcon_val=L_con(skip_feats, vgg_gt, alpha=Alpha_Lcon)\n",
        "  else:\n",
        "    Lcon_val=zero\n",
        "\n",
        "  if use_LP:\n",
        "    Lp_val=L_P(y_gen, y_gt)\n",
        "  else:\n",
        "    Lp_val=zero\n",
        "\n",
        "  if use_L1:\n",
        "    L1_val=L1(y_gen, y_gt)\n",
        "  else:\n",
        "    L1_val=zero\n",
        "\n",
        "  if use_Ltv:\n",
        "    Ltv_val=L_tv(y_gen, y_gt)\n",
        "  else:\n",
        "    Ltv_val=zero\n",
        "\n",
        "  if use_Lmtv and (mask is not None):\n",
        "    Lmtv_val=L_mtv(y_gen, y_gt, mask)\n",
        "  else:\n",
        "    Lmtv_val=zero\n",
        "\n",
        "  if use_Lssim:\n",
        "    Lssim_val=L_SSIM(y_gen, y_gt)\n",
        "  else:\n",
        "    Lssim_val=zero\n",
        "\n",
        "  LG_val=L_adv+ Lcon_val +Lambda_P*Lp_val +Lambda_L1*L1_val +Lambda_Tv*Ltv_val +Lambda_Mtv*Lmtv_val+Lambda_Ssim*Lssim_val\n",
        "\n",
        "  parts={\n",
        "    \"L_adv\":float(L_adv.item()),\n",
        "    \"L_con\":float(Lcon_val.item()),\n",
        "    \"L_P\":float(Lp_val.item()),\n",
        "    \"L_1\":float(L1_val.item()),\n",
        "    \"L_tv\":float(Ltv_val.item()),\n",
        "    \"L_m_tv\":float(Lmtv_val.item()),\n",
        "    \"L_SSIM\":float(Lssim_val.item()),\n",
        "    \"L_total\":float(LG_val.item()),\n",
        "  }\n",
        "  return LG_val, parts\n",
        "\n"
      ],
      "metadata": {
        "id": "E4-SMs7mm_Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Evaluation"
      ],
      "metadata": {
        "id": "glrCdioXpTtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have to put Path to TIRed dataset to run the code"
      ],
      "metadata": {
        "id": "EmrHw-oZp7xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Path = \"/path/to/TIReD\""
      ],
      "metadata": {
        "id": "mLrlusF9p4ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TIRed contains 4 subfolders: AVA, COCO, HKU-IS, Waterloo Exploration\n",
        "\n",
        "Each subfolder contains subfolders:\n",
        "\n",
        "train subfolder containing subfolders: tain_A: original image, tain_B: ground truth retargeted image\n",
        "\n",
        "test subfolder containing subfolder: test_A: original image, test_B: ground truth retargeted image"
      ],
      "metadata": {
        "id": "90LTcCh1rWYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ava_train=TIRed(\n",
        "  os.path.join(Path, \"AVA\", \"train\", \"train_A\"),\n",
        "  os.path.join(Path, \"AVA\", \"train\", \"train_B\"),\n",
        "  os.path.join(Path, \"AVA\", \"train\", \"train_B_mask\"),\n",
        ")\n",
        "\n",
        "coco224_train=TIRed(\n",
        "  os.path.join(Path, \"COCO\", \"train_224_224\", \"train_A\"),\n",
        "  os.path.join(Path, \"COCO\", \"train_224_224\", \"train_B\"),\n",
        "  os.path.join(Path, \"COCO\", \"train_224_224\", \"train_B_mask\"),\n",
        ")\n",
        "\n",
        "coco300_train=TIRed(\n",
        "  os.path.join(Path, \"COCO\", \"train_300_300\", \"train_A\"),\n",
        "  os.path.join(Path, \"COCO\", \"train_300_300\", \"train_B\"),\n",
        "  None,\n",
        ")\n",
        "\n",
        "hku_train=TIRed(\n",
        "  os.path.join(Path, \"HKU-IS\", \"train\", \"train_A\"),\n",
        "  os.path.join(Path, \"HKU-IS\", \"train\", \"train_B\"),\n",
        "  os.path.join(Path, \"HKU-IS\", \"train\", \"train_B_mask\"),\n",
        ")\n",
        "\n",
        "waterloo_train=TIRed(\n",
        "  os.path.join(Path, \"Waterloo Exploration\", \"train\", \"train_A\"),\n",
        "  os.path.join(Path, \"Waterloo Exploration\", \"train\", \"train_B\"),\n",
        "  os.path.join(Path, \"Waterloo Exploration\", \"train\", \"train_B_mask\"),\n",
        ")\n",
        "\n",
        "full_train_dataset=ConcatDataset(\n",
        "  [ava_train, coco224_train, coco300_train, hku_train, waterloo_train]\n",
        ")\n",
        "print(\"Total train images:\", len(full_train_dataset))\n"
      ],
      "metadata": {
        "id": "G4WineG_qVfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and validation"
      ],
      "metadata": {
        "id": "Dwp0i2qSv34W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_val= int(0.05 * len(full_train_dataset))\n",
        "n_train= len(full_train_dataset) - n_val\n",
        "\n",
        "train_ds, val_ds= random_split(full_train_dataset, [n_train, n_val], generator=torch.Generator().manual_seed(123),)\n",
        "\n",
        "batch_size= 8\n",
        "num_workers= 8\n",
        "\n",
        "train_loader= DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True,)\n",
        "\n",
        "val_loader= DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True,)"
      ],
      "metadata": {
        "id": "pckFgy9cv1h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset"
      ],
      "metadata": {
        "id": "ET-6Dg3l5qcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loaders: Dict[str, DataLoader] = {}\n",
        "\n",
        "# AVA\n",
        "ava_A= os.path.join(Path, \"AVA\", \"test\", \"test_A\")\n",
        "ava_B= os.path.join(Path, \"AVA\", \"test\", \"test_B\")\n",
        "ava_test= TIRed(ava_A, ava_B, None, image_size=224)\n",
        "test_loaders[\"AVA\"] = DataLoader(ava_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True,)\n",
        "print(\"AVA:\", len(ava_test), \"test images\")\n",
        "\n",
        "# COCO\n",
        "coco_A= os.path.join(Path, \"COCO\", \"test\", \"test_A\")\n",
        "coco_B= os.path.join(Path, \"COCO\", \"test\", \"test_B\")\n",
        "coco_test= TIRed(coco_A, coco_B, None, image_size=224)\n",
        "test_loaders[\"COCO\"] = DataLoader(coco_test,  batch_size=batch_size, shuffle=False,  num_workers=num_workers, pin_memory=True,)\n",
        "print(\"COCO:\", len(coco_test), \"test images\")\n",
        "\n",
        "# HKU-IS\n",
        "hku_A= os.path.join(Path, \"HKU-IS\", \"test\", \"test_A\")\n",
        "hku_B= os.path.join(Path, \"HKU-IS\", \"test\", \"test_B\")\n",
        "hku_test= TIRed(hku_A, hku_B, None, image_size=224)\n",
        "test_loaders[\"HKU-IS\"] = DataLoader( hku_test, batch_size=batch_size,  shuffle=False, num_workers=num_workers, pin_memory=True,)\n",
        "print(\"HKU-IS:\", len(hku_test), \"test images\")\n",
        "\n",
        "# Waterloo\n",
        "wat_A= os.path.join(Path, \"Waterloo Exploration\", \"test\", \"test_A\")\n",
        "wat_B= os.path.join(Path, \"Waterloo Exploration\", \"test\", \"test_B\")\n",
        "wat_test= TIRed(wat_A, wat_B, None, image_size=224)\n",
        "test_loaders[\"Waterloo\"] = DataLoader(wat_test,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True,)\n",
        "print(\"Waterloo:\", len(wat_test), \"test images\")\n",
        "\n",
        "# all_test_loader\n",
        "from torch.utils.data import ConcatDataset as _Concat\n",
        "\n",
        "all_test_dataset= _Concat([dl.dataset for dl in test_loaders.values()])\n",
        "all_test_loader= DataLoader(all_test_dataset, batch_size=batch_size,shuffle=False, num_workers=num_workers, pin_memory=True,)\n"
      ],
      "metadata": {
        "id": "johX2U8L5sWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "1y-tN1Qu-aaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "G= Generator().to(device)\n",
        "D= Discriminator().to(device)\n",
        "\n",
        "opt_G= torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "opt_D= torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "vgg19.to(device)\n",
        "vgg19.eval()"
      ],
      "metadata": {
        "id": "acgUn0Hy-bfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper for training and evaluation"
      ],
      "metadata": {
        "id": "N8-mhixLFCIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(loader, G, device, max_batches=None):\n",
        "    G.eval()\n",
        "    sums={\n",
        "        \"psnr_in\": 0.0,\n",
        "        \"psnr_out\": 0.0,\n",
        "        \"ssim_in\": 0.0,\n",
        "        \"ssim_out\": 0.0,\n",
        "        \"fsim_in\": 0.0,\n",
        "        \"fsim_out\": 0.0,\n",
        "        \"vif_in\": 0.0,\n",
        "        \"vif_out\": 0.0,\n",
        "    }\n",
        "    n_batches = 0\n",
        "\n",
        "    for i, (x, y, m) in enumerate(loader):\n",
        "        if (max_batches is not None) and (i >= max_batches):\n",
        "            break\n",
        "\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        y_pred = G(x)\n",
        "\n",
        "        x_dn = torch.clamp(denorm_imagenet(x), 0.0, 1.0)\n",
        "        y_dn = torch.clamp(denorm_imagenet(y), 0.0, 1.0)\n",
        "        y_pred_dn = torch.clamp(denorm_imagenet(y_pred), 0.0, 1.0)\n",
        "\n",
        "        sums[\"psnr_in\"]+= piq.psnr(x_dn, y_dn, data_range=1.0).item()\n",
        "        sums[\"psnr_out\"]+= piq.psnr(y_pred_dn, y_dn, data_range=1.0).item()\n",
        "\n",
        "        sums[\"ssim_in\"]+= piq.ssim(x_dn, y_dn, data_range=1.0).item()\n",
        "        sums[\"ssim_out\"]+= piq.ssim(y_pred_dn, y_dn, data_range=1.0).item()\n",
        "\n",
        "        sums[\"fsim_in\"]+= piq.fsim(x_dn, y_dn, data_range=1.0).item()\n",
        "        sums[\"fsim_out\"]+= piq.fsim(y_pred_dn, y_dn, data_range=1.0).item()\n",
        "\n",
        "        sums[\"vif_in\"] += piq.vif_p(x_dn, y_dn, data_range=1.0).item()\n",
        "        sums[\"vif_out\"] += piq.vif_p(y_pred_dn, y_dn, data_range=1.0).item()\n",
        "\n",
        "        n_batches += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return {k: float(\"nan\") for k in sums}\n",
        "\n",
        "    return {k: v / n_batches for k, v in sums.items()}\n",
        "\n",
        "\n",
        "def print_metrics(name, metrics):\n",
        "    print(\n",
        "        f\"{name:10s} | \"\n",
        "        f\"PSNR in/out: {metrics['psnr_in']:.3f} / {metrics['psnr_out']:.3f}  | \"\n",
        "        f\"SSIM in/out: {metrics['ssim_in']:.4f} / {metrics['ssim_out']:.4f}  | \"\n",
        "        f\"FSIM in/out: {metrics['fsim_in']:.4f} / {metrics['fsim_out']:.4f}  | \"\n",
        "        f\"VIF  in/out: {metrics['vif_in']:.4f} / {metrics['vif_out']:.4f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "gdm7emz7FD1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "oBCHCFtY_-wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=50\n",
        "best_val_ssim=-1.0\n",
        "\n",
        "best_path=f\"mrgan_{loss_mode}_best.pth\"\n",
        "\n",
        "train_history={\"G\":[], \"D\":[]}\n",
        "val_history=[]\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  G.train()\n",
        "  D.train()\n",
        "\n",
        "  error_G=0.0\n",
        "  error_D=0.0\n",
        "  steps=0\n",
        "\n",
        "  data=tqdm(train_loader, desc=f\"[{loss_mode}] Epoch {epoch}/{epochs}\", ncols=120)\n",
        "\n",
        "  for x, y, m in data:\n",
        "    x=x.to(device)\n",
        "    y=y.to(device)\n",
        "    m=m.to(device)\n",
        "\n",
        "    # Update D\n",
        "    with torch.no_grad():\n",
        "      y_fake_temp=G(x)\n",
        "\n",
        "    d_real=D(torch.cat([x, y], dim=1))\n",
        "    d_fake=D(torch.cat([x, y_fake_temp], dim=1))\n",
        "\n",
        "    loss_D=L_D(d_real, d_fake)\n",
        "    opt_D.zero_grad(set_to_none=True)\n",
        "    loss_D.backward()\n",
        "    opt_D.step()\n",
        "\n",
        "    # Update G\n",
        "    y_fake, skip_feats=G(x, return_skips=True)\n",
        "    d_fake=D(torch.cat([x, y_fake], dim=1))\n",
        "\n",
        "    loss_G, parts_G=L_G(y_fake, y, m, d_fake, skip_feats)\n",
        "    opt_G.zero_grad(set_to_none=True)\n",
        "    loss_G.backward()\n",
        "    opt_G.step()\n",
        "\n",
        "    steps+=1\n",
        "    error_G+=loss_G.item()\n",
        "    error_D+=loss_D.item()\n",
        "\n",
        "    if steps%50==0:\n",
        "      data.set_postfix({\n",
        "        \"G_loss\":f\"{error_G/steps:.3f}\",\n",
        "        \"D_loss\":f\"{error_D/steps:.3f}\",\n",
        "      })\n",
        "\n",
        "  avg_G=error_G/max(1, steps)\n",
        "  avg_D=error_D/max(1, steps)\n",
        "  train_history[\"G\"].append(avg_G)\n",
        "  train_history[\"D\"].append(avg_D)\n",
        "\n",
        "  val_metrics=evaluate(val_loader, G, device)\n",
        "  val_history.append(val_metrics)\n",
        "  val_ssim_out=val_metrics[\"ssim_out\"]\n",
        "\n",
        "  print(\n",
        "    f\"\\nEpoch {epoch:3d}/{epochs} ({loss_mode}): \"\n",
        "    f\"G_loss={avg_G:.4f}, D_loss={avg_D:.4f}, \"\n",
        "    f\"Val PSNR_out={val_metrics['psnr_out']:.3f} dB, \"\n",
        "    f\"Val SSIM_out={val_ssim_out:.4f}\"\n",
        "  )\n",
        "\n",
        "  if val_ssim_out>best_val_ssim:\n",
        "    best_val_ssim=val_ssim_out\n",
        "    torch.save(\n",
        "      {\n",
        "        \"epoch\":epoch,\n",
        "        \"G_state\":G.state_dict(),\n",
        "        \"D_state\":D.state_dict(),\n",
        "        \"opt_G_state\":opt_G.state_dict(),\n",
        "        \"opt_D_state\":opt_D.state_dict(),\n",
        "        \"val_metrics\":val_metrics,\n",
        "        \"loss_mode\":loss_mode,\n",
        "      },\n",
        "      best_path,\n",
        "    )\n",
        "    print(f\"  -> saved new best {best_path} with SSIM_out = {best_val_ssim:.4f}\")\n"
      ],
      "metadata": {
        "id": "Mjd3T84rAApI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outputs"
      ],
      "metadata": {
        "id": "A_R2_UcO-ou0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best=torch.load(best_path, map_location=device)\n",
        "G.load_state_dict(best[\"G_state\"])\n",
        "print(\n",
        "  f\"\\nLoaded best model from epoch {best['epoch']} \"\n",
        "  f\"(val SSIM_out = {best['val_metrics']['ssim_out']:.4f}, mode = {loss_mode})\"\n",
        ")\n",
        "\n",
        "G.eval()\n",
        "\n",
        "print(\"\\nPer-dataset TIReD test metrics (PSNR / SSIM / FSIM / VIF)\")\n",
        "for name, loader in test_loaders.items():\n",
        "  m=evaluate(loader, G, device)\n",
        "  print_metrics(name, m)\n",
        "\n",
        "m_all=evaluate(all_test_loader, G, device)\n",
        "print(\"\\nOverall TIReD test set\")\n",
        "print_metrics(\"TIReD_all\", m_all)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_outputs(G, loaders_dict, device, out_root=\"mrgan_examples\", per_dataset=8):\n",
        "  G.eval()\n",
        "  os.makedirs(out_root, exist_ok=True)\n",
        "\n",
        "  for name, loader in loaders_dict.items():\n",
        "    save_dir=os.path.join(out_root, name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving outputs for {name} into {save_dir} ...\")\n",
        "    saved=0\n",
        "\n",
        "    for x, y, m in loader:\n",
        "      if saved>=per_dataset:\n",
        "        break\n",
        "\n",
        "      x=x.to(device)\n",
        "      y=y.to(device)\n",
        "      y_pred=G(x)\n",
        "\n",
        "      bs=x.size(0)\n",
        "      for i in range(bs):\n",
        "        if saved>=per_dataset:\n",
        "          break\n",
        "\n",
        "        x_i=torch.clamp(\n",
        "          denorm_imagenet(x[i:i+1]), 0.0, 1.0\n",
        "        )[0].cpu().permute(1,2,0).numpy()\n",
        "        y_i=torch.clamp(\n",
        "          denorm_imagenet(y[i:i+1]), 0.0, 1.0\n",
        "        )[0].cpu().permute(1,2,0).numpy()\n",
        "        y_pred_i=torch.clamp(\n",
        "          denorm_imagenet(y_pred[i:i+1]), 0.0, 1.0\n",
        "        )[0].cpu().permute(1,2,0).numpy()\n",
        "\n",
        "        fig, axes=plt.subplots(1,3, figsize=(9,3))\n",
        "        axes[0].imshow(x_i);      axes[0].set_title(\"Input\");         axes[0].axis(\"off\")\n",
        "        axes[1].imshow(y_i);      axes[1].set_title(\"GT Retargeted\"); axes[1].axis(\"off\")\n",
        "        axes[2].imshow(y_pred_i); axes[2].set_title(\"MRGAN Output\");  axes[2].axis(\"off\")\n",
        "\n",
        "        fname=os.path.join(save_dir, f\"{name}_{saved:03d}_{loss_mode}.png\")\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(fname, dpi=150, bbox_inches=\"tight\")\n",
        "        plt.close(fig)\n",
        "\n",
        "        saved+=1\n",
        "\n",
        "    print(f\"  -> saved {saved} examples for {name}\\n\")\n",
        "\n",
        "\n",
        "save_outputs(G, test_loaders, device,\n",
        "             out_root=\"mrgan_examples\",\n",
        "             per_dataset=8)\n"
      ],
      "metadata": {
        "id": "vO9JUd4i9mNR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}